{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd4a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probability from s2 to s0 after playing action a1: 0.8\n",
      "Reward for transitioning from s2 to s0 after playing action a1: 40\n",
      "Possible actions in state s2: [1]\n"
     ]
    }
   ],
   "source": [
    "# From state s, after playing action a, the probability of reaching state s'\n",
    "transition_probabilities = [ # shape=[s, a, s']\n",
    "[[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "[[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "[None, [0.8, 0.1, 0.1], None]]\n",
    "\n",
    "# The corresponding reward, we will look up rewards[2][1][0] (which is +40)\n",
    "rewards = [ # shape=[s, a, s']\n",
    "[[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "[[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "[[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "\n",
    "# To get the list of possible actions in s2, we will look up possible_actions[2] (in this case, only action a1 is possible)\n",
    "# Possible actions in each state, indexed by state\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
    "\n",
    "# Transition probability from s2 to s0 after playing action a1, we will look up transition_probabilities[2][1][0] (which is 0.8).\n",
    "print(\"Transition probability from s2 to s0 after playing action a1: \" + str(transition_probabilities[2][1][0])) # 0.8\n",
    "# The corresponding reward, we will look up rewards[2][1][0] (which is +40)\n",
    "print(\"Reward for transitioning from s2 to s0 after playing action a1: \" + str(rewards[2][1][0])) # +40\n",
    "# Possible actions in state s2\n",
    "print(\"Possible actions in state s2: \" + str(possible_actions[2])) # [1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02df8466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Q-values:\n",
      " [[  0.   0.   0.]\n",
      " [  0. -inf   0.]\n",
      " [-inf   0. -inf]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Initialize all the Q-Values to 0 (except for the the impossible actions, for which we set the Q-Values to –∞)\n",
    "# Shape is [number of states, number of actions]\n",
    "Q_values = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0\n",
    "\n",
    "print(\"Initial Q-values:\\n\", Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b5d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-values after 50 iterations:\n",
      " [[18.91891892 17.02702703 13.62162162]\n",
      " [ 0.                -inf -4.87971488]\n",
      " [       -inf 50.13365013        -inf]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):  # For each state\n",
    "        for a in possible_actions[s]: # For each possible action in state s\n",
    "            Q_values[s, a] = np.sum([\n",
    "                transition_probabilities[s][a][sp] * (rewards[s][a][sp] +\n",
    "                gamma * np.max(Q_prev[sp])) \n",
    "                for sp in range(3)])\n",
    "            \n",
    "    #print(f\"Q-values after iteration {iteration + 1}:\\n\", Q_values)\n",
    "\n",
    "print(\"Final Q-values after 50 iterations:\\n\", Q_values)\n",
    "            \n",
    "# For example, when the agent is in state s0 and it chooses action a1, the expected sum\n",
    "# of discounted future rewards is approximately 17.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86bfb651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each state, let’s look at the action that has the highest Q-Value:\n",
    "np.argmax(Q_values, axis=1)  # This will give us the best action for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12ce09",
   "metadata": {},
   "source": [
    "This gives us the optimal policy for this MDP, when using a discount factor of 0.90: \n",
    "\n",
    "in state s0 choose action a0; \n",
    "\n",
    "in state s1 choose action a0 (i.e., stay put); \n",
    "\n",
    "and in state s2 choose action a1 (the only possible action). \n",
    "\n",
    "Interestingly, if we increase the discount\n",
    "factor to 0.95, the optimal policy changes: in state s1 the best action becomes a2 (go\n",
    "through the fire!). This makes sense because the more you value future rewards, the\n",
    "more you are willing to put up with some pain now for the promise of future bliss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f25f8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlearning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
