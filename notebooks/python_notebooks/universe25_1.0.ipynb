{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e98c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76294028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Generate the Fake Dataset ---\n",
    "\n",
    "def create_fake_survival_data(num_samples=10000):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset for mouse survival based on a set of rules.\n",
    "    \"\"\"\n",
    "    # Generate random base features\n",
    "    x_position = np.random.rand(num_samples) * 100  # Env size 100x100\n",
    "    y_position = np.random.rand(num_samples) * 100\n",
    "    hunger_level = np.random.rand(num_samples)      # 0 to 1\n",
    "    stress_level = np.random.rand(num_samples)      # 0 to 1\n",
    "    density = np.random.rand(num_samples) * 50      # 0 to 50 mice nearby\n",
    "\n",
    "    # Combine features into a single matrix (num_samples x 5 features)\n",
    "    # The columns are the features, and the rows are different mice (samples).\n",
    "    features = np.stack([\n",
    "        x_position, y_position, hunger_level, stress_level, density\n",
    "    ], axis=1)\n",
    "\n",
    "    # print(features)\n",
    "\n",
    "    # Generate labels based on our \"rule of nature\"\n",
    "    # Label 1 = dead, Label 0 = alive\n",
    "    # Start by assuming all are alive\n",
    "    labels = np.zeros(num_samples)\n",
    "\n",
    "    # Rule 1: High stress AND high hunger is very bad.\n",
    "    condition1 = (stress_level > 0.8) & (hunger_level > 0.85)\n",
    "\n",
    "    # Rule 2: Extreme population density is very bad.\n",
    "    condition2 = density > 45\n",
    "\n",
    "    # Apply these rules to set the labels for \"dead\" mice\n",
    "    labels[condition1 | condition2] = 1\n",
    "\n",
    "        \n",
    "    # Add a little bit of random noise to make it more realistic\n",
    "    # (Sometimes a healthy mouse dies, or a stressed one survives)\n",
    "    noise = np.random.rand(num_samples) < 0.05 # 5% chance of flipping the label\n",
    "    labels = np.abs(labels - noise)\n",
    "\n",
    "    # print(labels)\n",
    "\n",
    "\n",
    "    print(f\"Generated {num_samples} samples.\")\n",
    "    print(f\"Number of 'alive' mice (0): {np.count_nonzero(labels == 0)}\")\n",
    "    print(f\"Number of 'dead' mice (1): {np.count_nonzero(labels == 1)}\\n\")\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72943268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000 samples.\n",
      "Number of 'alive' mice (0): 83668\n",
      "Number of 'dead' mice (1): 16332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Prepare Data for PyTorch ---\n",
    "\n",
    "# Create the data\n",
    "features_np, labels_np = create_fake_survival_data(num_samples=100000)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch Tensors\n",
    "# The features are our 'X' and the labels are our 'y'\n",
    "features_tensor = torch.tensor(features_np, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels_np, dtype=torch.float32).unsqueeze(1) # Add a dimension for the loss function\n",
    "# print(labels_tensor)\n",
    "\n",
    "# Create a Dataset and DataLoader to handle batching\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feba5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurvivalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SurvivalClassifier, self).__init__()\n",
    "        # Input layer: 5 features (x, y, hunger, stress, density)\n",
    "        # Hidden layer 1: 16 neurons\n",
    "        # Hidden layer 2: 8 neurons\n",
    "        # Output layer: 1 neuron (a single value representing the prediction)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(5, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1) # Output is a single value, called a 'logit'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa012576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/200], Average Loss: 0.4332\n",
      "Epoch [2/200], Average Loss: 0.3784\n",
      "Epoch [3/200], Average Loss: 0.3528\n",
      "Epoch [4/200], Average Loss: 0.3343\n",
      "Epoch [5/200], Average Loss: 0.3188\n",
      "Epoch [6/200], Average Loss: 0.3050\n",
      "Epoch [7/200], Average Loss: 0.2943\n",
      "Epoch [8/200], Average Loss: 0.2859\n",
      "Epoch [9/200], Average Loss: 0.2802\n",
      "Epoch [10/200], Average Loss: 0.2753\n",
      "Epoch [11/200], Average Loss: 0.2704\n",
      "Epoch [12/200], Average Loss: 0.2683\n",
      "Epoch [13/200], Average Loss: 0.2644\n",
      "Epoch [14/200], Average Loss: 0.2607\n",
      "Epoch [15/200], Average Loss: 0.2577\n",
      "Epoch [16/200], Average Loss: 0.2565\n",
      "Epoch [17/200], Average Loss: 0.2537\n",
      "Epoch [18/200], Average Loss: 0.2522\n",
      "Epoch [19/200], Average Loss: 0.2507\n",
      "Epoch [20/200], Average Loss: 0.2497\n",
      "Epoch [21/200], Average Loss: 0.2471\n",
      "Epoch [22/200], Average Loss: 0.2469\n",
      "Epoch [23/200], Average Loss: 0.2460\n",
      "Epoch [24/200], Average Loss: 0.2456\n",
      "Epoch [25/200], Average Loss: 0.2453\n",
      "Epoch [26/200], Average Loss: 0.2458\n",
      "Epoch [27/200], Average Loss: 0.2440\n",
      "Epoch [28/200], Average Loss: 0.2442\n",
      "Epoch [29/200], Average Loss: 0.2415\n",
      "Epoch [30/200], Average Loss: 0.2426\n",
      "Epoch [31/200], Average Loss: 0.2426\n",
      "Epoch [32/200], Average Loss: 0.2418\n",
      "Epoch [33/200], Average Loss: 0.2416\n",
      "Epoch [34/200], Average Loss: 0.2424\n",
      "Epoch [35/200], Average Loss: 0.2410\n",
      "Epoch [36/200], Average Loss: 0.2394\n",
      "Epoch [37/200], Average Loss: 0.2414\n",
      "Epoch [38/200], Average Loss: 0.2396\n",
      "Epoch [39/200], Average Loss: 0.2402\n",
      "Epoch [40/200], Average Loss: 0.2395\n",
      "Epoch [41/200], Average Loss: 0.2388\n",
      "Epoch [42/200], Average Loss: 0.2400\n",
      "Epoch [43/200], Average Loss: 0.2398\n",
      "Epoch [44/200], Average Loss: 0.2385\n",
      "Epoch [45/200], Average Loss: 0.2394\n",
      "Epoch [46/200], Average Loss: 0.2389\n",
      "Epoch [47/200], Average Loss: 0.2391\n",
      "Epoch [48/200], Average Loss: 0.2390\n",
      "Epoch [49/200], Average Loss: 0.2388\n",
      "Epoch [50/200], Average Loss: 0.2377\n",
      "Epoch [51/200], Average Loss: 0.2382\n",
      "Epoch [52/200], Average Loss: 0.2377\n",
      "Epoch [53/200], Average Loss: 0.2377\n",
      "Epoch [54/200], Average Loss: 0.2382\n",
      "Epoch [55/200], Average Loss: 0.2380\n",
      "Epoch [56/200], Average Loss: 0.2373\n",
      "Epoch [57/200], Average Loss: 0.2373\n",
      "Epoch [58/200], Average Loss: 0.2361\n",
      "Epoch [59/200], Average Loss: 0.2371\n",
      "Epoch [60/200], Average Loss: 0.2368\n",
      "Epoch [61/200], Average Loss: 0.2377\n",
      "Epoch [62/200], Average Loss: 0.2372\n",
      "Epoch [63/200], Average Loss: 0.2364\n",
      "Epoch [64/200], Average Loss: 0.2361\n",
      "Epoch [65/200], Average Loss: 0.2364\n",
      "Epoch [66/200], Average Loss: 0.2367\n",
      "Epoch [67/200], Average Loss: 0.2365\n",
      "Epoch [68/200], Average Loss: 0.2363\n",
      "Epoch [69/200], Average Loss: 0.2354\n",
      "Epoch [70/200], Average Loss: 0.2368\n",
      "Epoch [71/200], Average Loss: 0.2360\n",
      "Epoch [72/200], Average Loss: 0.2358\n",
      "Epoch [73/200], Average Loss: 0.2359\n",
      "Epoch [74/200], Average Loss: 0.2359\n",
      "Epoch [75/200], Average Loss: 0.2360\n",
      "Epoch [76/200], Average Loss: 0.2352\n",
      "Epoch [77/200], Average Loss: 0.2359\n",
      "Epoch [78/200], Average Loss: 0.2349\n",
      "Epoch [79/200], Average Loss: 0.2349\n",
      "Epoch [80/200], Average Loss: 0.2351\n",
      "Epoch [81/200], Average Loss: 0.2353\n",
      "Epoch [82/200], Average Loss: 0.2361\n",
      "Epoch [83/200], Average Loss: 0.2347\n",
      "Epoch [84/200], Average Loss: 0.2346\n",
      "Epoch [85/200], Average Loss: 0.2348\n",
      "Epoch [86/200], Average Loss: 0.2353\n",
      "Epoch [87/200], Average Loss: 0.2354\n",
      "Epoch [88/200], Average Loss: 0.2338\n",
      "Epoch [89/200], Average Loss: 0.2349\n",
      "Epoch [90/200], Average Loss: 0.2353\n",
      "Epoch [91/200], Average Loss: 0.2343\n",
      "Epoch [92/200], Average Loss: 0.2346\n",
      "Epoch [93/200], Average Loss: 0.2337\n",
      "Epoch [94/200], Average Loss: 0.2346\n",
      "Epoch [95/200], Average Loss: 0.2348\n",
      "Epoch [96/200], Average Loss: 0.2335\n",
      "Epoch [97/200], Average Loss: 0.2339\n",
      "Epoch [98/200], Average Loss: 0.2343\n",
      "Epoch [99/200], Average Loss: 0.2335\n",
      "Epoch [100/200], Average Loss: 0.2338\n",
      "Epoch [101/200], Average Loss: 0.2340\n",
      "Epoch [102/200], Average Loss: 0.2338\n",
      "Epoch [103/200], Average Loss: 0.2327\n",
      "Epoch [104/200], Average Loss: 0.2335\n",
      "Epoch [105/200], Average Loss: 0.2331\n",
      "Epoch [106/200], Average Loss: 0.2334\n",
      "Epoch [107/200], Average Loss: 0.2327\n",
      "Epoch [108/200], Average Loss: 0.2336\n",
      "Epoch [109/200], Average Loss: 0.2318\n",
      "Epoch [110/200], Average Loss: 0.2321\n",
      "Epoch [111/200], Average Loss: 0.2313\n",
      "Epoch [112/200], Average Loss: 0.2318\n",
      "Epoch [113/200], Average Loss: 0.2314\n",
      "Epoch [114/200], Average Loss: 0.2308\n",
      "Epoch [115/200], Average Loss: 0.2308\n",
      "Epoch [116/200], Average Loss: 0.2310\n",
      "Epoch [117/200], Average Loss: 0.2303\n",
      "Epoch [118/200], Average Loss: 0.2302\n",
      "Epoch [119/200], Average Loss: 0.2307\n",
      "Epoch [120/200], Average Loss: 0.2299\n",
      "Epoch [121/200], Average Loss: 0.2301\n",
      "Epoch [122/200], Average Loss: 0.2296\n",
      "Epoch [123/200], Average Loss: 0.2295\n",
      "Epoch [124/200], Average Loss: 0.2295\n",
      "Epoch [125/200], Average Loss: 0.2294\n",
      "Epoch [126/200], Average Loss: 0.2293\n",
      "Epoch [127/200], Average Loss: 0.2284\n",
      "Epoch [128/200], Average Loss: 0.2290\n",
      "Epoch [129/200], Average Loss: 0.2285\n",
      "Epoch [130/200], Average Loss: 0.2281\n",
      "Epoch [131/200], Average Loss: 0.2280\n",
      "Epoch [132/200], Average Loss: 0.2280\n",
      "Epoch [133/200], Average Loss: 0.2282\n",
      "Epoch [134/200], Average Loss: 0.2278\n",
      "Epoch [135/200], Average Loss: 0.2280\n",
      "Epoch [136/200], Average Loss: 0.2290\n",
      "Epoch [137/200], Average Loss: 0.2281\n",
      "Epoch [138/200], Average Loss: 0.2278\n",
      "Epoch [139/200], Average Loss: 0.2277\n",
      "Epoch [140/200], Average Loss: 0.2276\n",
      "Epoch [141/200], Average Loss: 0.2276\n",
      "Epoch [142/200], Average Loss: 0.2274\n",
      "Epoch [143/200], Average Loss: 0.2267\n",
      "Epoch [144/200], Average Loss: 0.2273\n",
      "Epoch [145/200], Average Loss: 0.2271\n",
      "Epoch [146/200], Average Loss: 0.2274\n",
      "Epoch [147/200], Average Loss: 0.2272\n",
      "Epoch [148/200], Average Loss: 0.2270\n",
      "Epoch [149/200], Average Loss: 0.2274\n",
      "Epoch [150/200], Average Loss: 0.2262\n",
      "Epoch [151/200], Average Loss: 0.2265\n",
      "Epoch [152/200], Average Loss: 0.2268\n",
      "Epoch [153/200], Average Loss: 0.2274\n",
      "Epoch [154/200], Average Loss: 0.2273\n",
      "Epoch [155/200], Average Loss: 0.2270\n",
      "Epoch [156/200], Average Loss: 0.2275\n",
      "Epoch [157/200], Average Loss: 0.2272\n",
      "Epoch [158/200], Average Loss: 0.2256\n",
      "Epoch [159/200], Average Loss: 0.2262\n",
      "Epoch [160/200], Average Loss: 0.2272\n",
      "Epoch [161/200], Average Loss: 0.2257\n",
      "Epoch [162/200], Average Loss: 0.2269\n",
      "Epoch [163/200], Average Loss: 0.2269\n",
      "Epoch [164/200], Average Loss: 0.2262\n",
      "Epoch [165/200], Average Loss: 0.2266\n",
      "Epoch [166/200], Average Loss: 0.2265\n",
      "Epoch [167/200], Average Loss: 0.2258\n",
      "Epoch [168/200], Average Loss: 0.2266\n",
      "Epoch [169/200], Average Loss: 0.2264\n",
      "Epoch [170/200], Average Loss: 0.2264\n",
      "Epoch [171/200], Average Loss: 0.2264\n",
      "Epoch [172/200], Average Loss: 0.2262\n",
      "Epoch [173/200], Average Loss: 0.2264\n",
      "Epoch [174/200], Average Loss: 0.2260\n",
      "Epoch [175/200], Average Loss: 0.2265\n",
      "Epoch [176/200], Average Loss: 0.2264\n",
      "Epoch [177/200], Average Loss: 0.2263\n",
      "Epoch [178/200], Average Loss: 0.2267\n",
      "Epoch [179/200], Average Loss: 0.2263\n",
      "Epoch [180/200], Average Loss: 0.2259\n",
      "Epoch [181/200], Average Loss: 0.2257\n",
      "Epoch [182/200], Average Loss: 0.2259\n",
      "Epoch [183/200], Average Loss: 0.2255\n",
      "Epoch [184/200], Average Loss: 0.2261\n",
      "Epoch [185/200], Average Loss: 0.2255\n",
      "Epoch [186/200], Average Loss: 0.2261\n",
      "Epoch [187/200], Average Loss: 0.2260\n",
      "Epoch [188/200], Average Loss: 0.2253\n",
      "Epoch [189/200], Average Loss: 0.2262\n",
      "Epoch [190/200], Average Loss: 0.2256\n",
      "Epoch [191/200], Average Loss: 0.2267\n",
      "Epoch [192/200], Average Loss: 0.2256\n",
      "Epoch [193/200], Average Loss: 0.2251\n",
      "Epoch [194/200], Average Loss: 0.2256\n",
      "Epoch [195/200], Average Loss: 0.2253\n",
      "Epoch [196/200], Average Loss: 0.2257\n",
      "Epoch [197/200], Average Loss: 0.2256\n",
      "Epoch [198/200], Average Loss: 0.2261\n",
      "Epoch [199/200], Average Loss: 0.2254\n",
      "Epoch [200/200], Average Loss: 0.2260\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Train the Network ---\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 200\n",
    "learning_rate = 0.0005 \n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SurvivalClassifier()\n",
    "# BCEWithLogitsLoss is perfect for binary classification. It's numerically stable.\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# The training loop\n",
    "for epoch in range(epochs):\n",
    "    # *** FIX 3: TRACK AVERAGE EPOCH LOSS ***\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_features, batch_labels in data_loader:\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Training finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc7e865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Making Predictions on New Data ---\n",
      "Healthy Mouse -> Survival Probability: 0.0492, Prediction: Alive\n",
      "Stressed Mouse -> Survival Probability: 0.7331, Prediction: Dead\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Use the Trained Model for Prediction ---\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create two new test mice\n",
    "# Mouse 1: Should be ALIVE (low stress, low hunger, low density)\n",
    "mouse_healthy = torch.tensor([[50.0, 50.0, 0.1, 0.1, 5.0]], dtype=torch.float32)\n",
    "\n",
    "# Mouse 2: Should be DEAD (high stress, high hunger)\n",
    "mouse_stressed = torch.tensor([[25.0, 30.0, 0.9, 0.9, 15.0]], dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad(): # We don't need to calculate gradients for prediction\n",
    "    # Get the raw output (logit) from the model\n",
    "    healthy_logit = model(mouse_healthy)\n",
    "    stressed_logit = model(mouse_stressed)\n",
    "\n",
    "    # Convert the logit to a probability (0 to 1) using the sigmoid function\n",
    "    healthy_prob = torch.sigmoid(healthy_logit)\n",
    "    stressed_prob = torch.sigmoid(stressed_logit)\n",
    "\n",
    "    # Make a final decision\n",
    "    healthy_prediction = \"Alive\" if healthy_prob.item() < 0.5 else \"Dead\"\n",
    "    stressed_prediction = \"Alive\" if stressed_prob.item() < 0.5 else \"Dead\"\n",
    "\n",
    "print(\"--- Making Predictions on New Data ---\")\n",
    "print(f\"Healthy Mouse -> Survival Probability: {healthy_prob.item():.4f}, Prediction: {healthy_prediction}\")\n",
    "print(f\"Stressed Mouse -> Survival Probability: {stressed_prob.item():.4f}, Prediction: {stressed_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5de30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlearning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
